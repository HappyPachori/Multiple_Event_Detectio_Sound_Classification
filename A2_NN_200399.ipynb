{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ud6XZwOzqTbM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tensorflow.keras.models import load_model\n",
        "from keras.preprocessing import image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import models, layers\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nj0JzOR8qYAE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75857dd4-525f-4ea1-a54d-6d760437b8ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting google Drive to import data\n",
        "path = '/content/drive/MyDrive/Assignment_2_data/dataset'\n",
        "data_path=os.path.join(path, \"X\")\n",
        "label_path=os.path.join(path, \"Y\")"
      ],
      "metadata": {
        "id": "qXxg_SzluzBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.load('/content/drive/MyDrive/Assignment_2_data/x_train.npy')\n",
        "x_test = np.load('/content/drive/MyDrive/Assignment_2_data/x_test.npy')\n",
        "y_train = np.load('/content/drive/MyDrive/Assignment_2_data/y_train.npy')\n",
        "y_test = np.load('/content/drive/MyDrive/Assignment_2_data/y_test.npy')"
      ],
      "metadata": {
        "id": "nZZpKkSnuz5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape\n",
        "y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbwFXw301cFy",
        "outputId": "f0d47d66-39a2-41a4-db34-d84e6411e0c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape= x_train.shape\n",
        "ANNmodel = models.Sequential()\n",
        "ANNmodel.add(layers.Flatten())\n",
        "ANNmodel.add(layers.Dense(256, activation='relu'))\n",
        "ANNmodel.add(layers.Dropout(0.5))\n",
        "ANNmodel.add(layers.Dense(128, activation='relu'))\n",
        "ANNmodel.add(layers.Dropout(0.5))\n",
        "ANNmodel.add(layers.Dense(64, activation='relu'))\n",
        "ANNmodel.add(layers.Dropout(0.5))\n",
        "ANNmodel.add(layers.Dense(32, activation='relu'))\n",
        "ANNmodel.add(layers.Dropout(0.5))\n",
        "ANNmodel.add(layers.Dense(10, activation='sigmoid'))\n"
      ],
      "metadata": {
        "id": "mjmKkafgvEM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # `input_shape` is the shape of the input data\n",
        "ANNmodel.build(input_shape = input_shape)                        # e.g. input_shape = (None, 32, 32, 3)\n",
        "ANNmodel.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M32gE15q0Jjf",
        "outputId": "d4d986b3-8645-4652-e942-b81374e9a1cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_1 (Flatten)         (10000, 64000)            0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (10000, 256)              16384256  \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (10000, 256)              0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (10000, 128)              32896     \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (10000, 128)              0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (10000, 64)               8256      \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (10000, 64)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (10000, 32)               2080      \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (10000, 32)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (10000, 10)               330       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16,427,818\n",
            "Trainable params: 16,427,818\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ANNmodel.compile(optimizer='adam',loss=tf.keras.losses.BinaryCrossentropy(),metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "W4rgGYlhxaOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "filepath = '/content/drive/MyDrive/Assignment_2_data/ANN_best.epoch-loss.hdf5'\n",
        "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')"
      ],
      "metadata": {
        "id": "YlLwr2EHxejN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=ANNmodel.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=32, epochs=126, verbose = 1, callbacks=[checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uG15IFVxxg4f",
        "outputId": "c4b93d92-c9ba-4491-dba7-d44ed73b9db3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/126\n",
            "312/313 [============================>.] - ETA: 0s - loss: 0.5987 - accuracy: 0.1188\n",
            "Epoch 1: val_accuracy improved from -inf to 0.04800, saving model to /content/drive/MyDrive/Assignment_2_data/ANN_best.epoch-loss.hdf5\n",
            "313/313 [==============================] - 8s 15ms/step - loss: 0.5983 - accuracy: 0.1190 - val_loss: 0.4588 - val_accuracy: 0.0480\n",
            "Epoch 2/126\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.4156 - accuracy: 0.1125\n",
            "Epoch 2: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.4156 - accuracy: 0.1125 - val_loss: 0.4556 - val_accuracy: 0.0480\n",
            "Epoch 3/126\n",
            "308/313 [============================>.] - ETA: 0s - loss: 0.4031 - accuracy: 0.1128\n",
            "Epoch 3: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.4031 - accuracy: 0.1125 - val_loss: 0.4572 - val_accuracy: 0.0480\n",
            "Epoch 4/126\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.3976 - accuracy: 0.1123\n",
            "Epoch 4: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3976 - accuracy: 0.1125 - val_loss: 0.4583 - val_accuracy: 0.0480\n",
            "Epoch 5/126\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.3936 - accuracy: 0.1125\n",
            "Epoch 5: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.3936 - accuracy: 0.1125 - val_loss: 0.4572 - val_accuracy: 0.0480\n",
            "Epoch 6/126\n",
            "312/313 [============================>.] - ETA: 0s - loss: 0.3907 - accuracy: 0.1125\n",
            "Epoch 6: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.3907 - accuracy: 0.1125 - val_loss: 0.4581 - val_accuracy: 0.0480\n",
            "Epoch 7/126\n",
            "309/313 [============================>.] - ETA: 0s - loss: 0.3901 - accuracy: 0.1122\n",
            "Epoch 7: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.3900 - accuracy: 0.1125 - val_loss: 0.4565 - val_accuracy: 0.0480\n",
            "Epoch 8/126\n",
            "308/313 [============================>.] - ETA: 0s - loss: 0.3880 - accuracy: 0.1125\n",
            "Epoch 8: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.3878 - accuracy: 0.1125 - val_loss: 0.4600 - val_accuracy: 0.0480\n",
            "Epoch 9/126\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.3866 - accuracy: 0.1127\n",
            "Epoch 9: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.3867 - accuracy: 0.1125 - val_loss: 0.4581 - val_accuracy: 0.0480\n",
            "Epoch 10/126\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.3859 - accuracy: 0.1125\n",
            "Epoch 10: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3859 - accuracy: 0.1125 - val_loss: 0.4585 - val_accuracy: 0.0480\n",
            "Epoch 11/126\n",
            "310/313 [============================>.] - ETA: 0s - loss: 0.3852 - accuracy: 0.1121\n",
            "Epoch 11: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.3851 - accuracy: 0.1125 - val_loss: 0.4584 - val_accuracy: 0.0480\n",
            "Epoch 12/126\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.3844 - accuracy: 0.1125\n",
            "Epoch 12: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.3844 - accuracy: 0.1125 - val_loss: 0.4598 - val_accuracy: 0.0480\n",
            "Epoch 13/126\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.3839 - accuracy: 0.1119\n",
            "Epoch 13: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.3839 - accuracy: 0.1125 - val_loss: 0.4589 - val_accuracy: 0.0480\n",
            "Epoch 14/126\n",
            "309/313 [============================>.] - ETA: 0s - loss: 0.3830 - accuracy: 0.1122\n",
            "Epoch 14: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.3830 - accuracy: 0.1125 - val_loss: 0.4599 - val_accuracy: 0.0480\n",
            "Epoch 15/126\n",
            "310/313 [============================>.] - ETA: 0s - loss: 0.3830 - accuracy: 0.1127\n",
            "Epoch 15: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.3831 - accuracy: 0.1125 - val_loss: 0.4591 - val_accuracy: 0.0480\n",
            "Epoch 16/126\n",
            "308/313 [============================>.] - ETA: 0s - loss: 0.3826 - accuracy: 0.1125\n",
            "Epoch 16: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.3825 - accuracy: 0.1125 - val_loss: 0.4586 - val_accuracy: 0.0480\n",
            "Epoch 17/126\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.3821 - accuracy: 0.1125\n",
            "Epoch 17: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.3821 - accuracy: 0.1125 - val_loss: 0.4605 - val_accuracy: 0.0480\n",
            "Epoch 18/126\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.3818 - accuracy: 0.1122\n",
            "Epoch 18: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.3818 - accuracy: 0.1125 - val_loss: 0.4605 - val_accuracy: 0.0480\n",
            "Epoch 19/126\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.3816 - accuracy: 0.1125\n",
            "Epoch 19: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3816 - accuracy: 0.1125 - val_loss: 0.4600 - val_accuracy: 0.0480\n",
            "Epoch 20/126\n",
            "309/313 [============================>.] - ETA: 0s - loss: 0.3813 - accuracy: 0.1127\n",
            "Epoch 20: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3814 - accuracy: 0.1125 - val_loss: 0.4585 - val_accuracy: 0.0480\n",
            "Epoch 21/126\n",
            "309/313 [============================>.] - ETA: 0s - loss: 0.3810 - accuracy: 0.1120\n",
            "Epoch 21: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3809 - accuracy: 0.1125 - val_loss: 0.4595 - val_accuracy: 0.0480\n",
            "Epoch 22/126\n",
            "309/313 [============================>.] - ETA: 0s - loss: 0.3809 - accuracy: 0.1128\n",
            "Epoch 22: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 4s 13ms/step - loss: 0.3809 - accuracy: 0.1125 - val_loss: 0.4603 - val_accuracy: 0.0480\n",
            "Epoch 23/126\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.3807 - accuracy: 0.1125\n",
            "Epoch 23: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3807 - accuracy: 0.1125 - val_loss: 0.4594 - val_accuracy: 0.0480\n",
            "Epoch 24/126\n",
            "312/313 [============================>.] - ETA: 0s - loss: 0.3806 - accuracy: 0.1125\n",
            "Epoch 24: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.3806 - accuracy: 0.1125 - val_loss: 0.4596 - val_accuracy: 0.0480\n",
            "Epoch 25/126\n",
            "308/313 [============================>.] - ETA: 0s - loss: 0.3802 - accuracy: 0.1126\n",
            "Epoch 25: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3804 - accuracy: 0.1125 - val_loss: 0.4589 - val_accuracy: 0.0480\n",
            "Epoch 26/126\n",
            "310/313 [============================>.] - ETA: 0s - loss: 0.3803 - accuracy: 0.1124\n",
            "Epoch 26: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.3803 - accuracy: 0.1125 - val_loss: 0.4591 - val_accuracy: 0.0480\n",
            "Epoch 27/126\n",
            "310/313 [============================>.] - ETA: 0s - loss: 0.3802 - accuracy: 0.1122\n",
            "Epoch 27: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3802 - accuracy: 0.1125 - val_loss: 0.4597 - val_accuracy: 0.0480\n",
            "Epoch 28/126\n",
            "309/313 [============================>.] - ETA: 0s - loss: 0.3800 - accuracy: 0.1129\n",
            "Epoch 28: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.3800 - accuracy: 0.1125 - val_loss: 0.4610 - val_accuracy: 0.0480\n",
            "Epoch 29/126\n",
            "309/313 [============================>.] - ETA: 0s - loss: 0.3798 - accuracy: 0.1123\n",
            "Epoch 29: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3798 - accuracy: 0.1125 - val_loss: 0.4614 - val_accuracy: 0.0480\n",
            "Epoch 30/126\n",
            "308/313 [============================>.] - ETA: 0s - loss: 0.3801 - accuracy: 0.1131\n",
            "Epoch 30: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3799 - accuracy: 0.1125 - val_loss: 0.4600 - val_accuracy: 0.0480\n",
            "Epoch 31/126\n",
            "309/313 [============================>.] - ETA: 0s - loss: 0.3799 - accuracy: 0.1129\n",
            "Epoch 31: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.3798 - accuracy: 0.1125 - val_loss: 0.4589 - val_accuracy: 0.0480\n",
            "Epoch 32/126\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.3797 - accuracy: 0.1126\n",
            "Epoch 32: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.3797 - accuracy: 0.1125 - val_loss: 0.4602 - val_accuracy: 0.0480\n",
            "Epoch 33/126\n",
            "309/313 [============================>.] - ETA: 0s - loss: 0.3796 - accuracy: 0.1131\n",
            "Epoch 33: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.3797 - accuracy: 0.1125 - val_loss: 0.4607 - val_accuracy: 0.0480\n",
            "Epoch 34/126\n",
            "309/313 [============================>.] - ETA: 0s - loss: 0.3794 - accuracy: 0.1119\n",
            "Epoch 34: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.3794 - accuracy: 0.1125 - val_loss: 0.4607 - val_accuracy: 0.0480\n",
            "Epoch 35/126\n",
            "309/313 [============================>.] - ETA: 0s - loss: 0.3796 - accuracy: 0.1127\n",
            "Epoch 35: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3796 - accuracy: 0.1125 - val_loss: 0.4603 - val_accuracy: 0.0480\n",
            "Epoch 36/126\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.3796 - accuracy: 0.1125\n",
            "Epoch 36: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 0.3796 - accuracy: 0.1125 - val_loss: 0.4592 - val_accuracy: 0.0480\n",
            "Epoch 37/126\n",
            "309/313 [============================>.] - ETA: 0s - loss: 0.3795 - accuracy: 0.1130\n",
            "Epoch 37: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3794 - accuracy: 0.1125 - val_loss: 0.4595 - val_accuracy: 0.0480\n",
            "Epoch 38/126\n",
            "309/313 [============================>.] - ETA: 0s - loss: 0.3793 - accuracy: 0.1126\n",
            "Epoch 38: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3794 - accuracy: 0.1125 - val_loss: 0.4596 - val_accuracy: 0.0480\n",
            "Epoch 39/126\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.3794 - accuracy: 0.1122\n",
            "Epoch 39: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.3794 - accuracy: 0.1125 - val_loss: 0.4588 - val_accuracy: 0.0480\n",
            "Epoch 40/126\n",
            "309/313 [============================>.] - ETA: 0s - loss: 0.3793 - accuracy: 0.1125\n",
            "Epoch 40: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.3793 - accuracy: 0.1125 - val_loss: 0.4607 - val_accuracy: 0.0480\n",
            "Epoch 41/126\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.3791 - accuracy: 0.1124\n",
            "Epoch 41: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3792 - accuracy: 0.1125 - val_loss: 0.4601 - val_accuracy: 0.0480\n",
            "Epoch 42/126\n",
            "312/313 [============================>.] - ETA: 0s - loss: 0.3792 - accuracy: 0.1123\n",
            "Epoch 42: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3792 - accuracy: 0.1125 - val_loss: 0.4603 - val_accuracy: 0.0480\n",
            "Epoch 43/126\n",
            "309/313 [============================>.] - ETA: 0s - loss: 0.3791 - accuracy: 0.1125\n",
            "Epoch 43: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.3792 - accuracy: 0.1125 - val_loss: 0.4603 - val_accuracy: 0.0480\n",
            "Epoch 44/126\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.3791 - accuracy: 0.1122\n",
            "Epoch 44: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.3791 - accuracy: 0.1125 - val_loss: 0.4614 - val_accuracy: 0.0480\n",
            "Epoch 45/126\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.3790 - accuracy: 0.1126\n",
            "Epoch 45: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.3790 - accuracy: 0.1125 - val_loss: 0.4596 - val_accuracy: 0.0480\n",
            "Epoch 46/126\n",
            "312/313 [============================>.] - ETA: 0s - loss: 0.3790 - accuracy: 0.1124\n",
            "Epoch 46: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.3790 - accuracy: 0.1125 - val_loss: 0.4608 - val_accuracy: 0.0480\n",
            "Epoch 47/126\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.3790 - accuracy: 0.1121\n",
            "Epoch 47: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.3790 - accuracy: 0.1125 - val_loss: 0.4609 - val_accuracy: 0.0480\n",
            "Epoch 48/126\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.3792 - accuracy: 0.1121\n",
            "Epoch 48: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3791 - accuracy: 0.1125 - val_loss: 0.4604 - val_accuracy: 0.0480\n",
            "Epoch 49/126\n",
            "309/313 [============================>.] - ETA: 0s - loss: 0.3790 - accuracy: 0.1126\n",
            "Epoch 49: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3789 - accuracy: 0.1125 - val_loss: 0.4608 - val_accuracy: 0.0480\n",
            "Epoch 50/126\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.3789 - accuracy: 0.1125\n",
            "Epoch 50: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.3789 - accuracy: 0.1125 - val_loss: 0.4592 - val_accuracy: 0.0480\n",
            "Epoch 51/126\n",
            "308/313 [============================>.] - ETA: 0s - loss: 0.3790 - accuracy: 0.1124\n",
            "Epoch 51: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.3790 - accuracy: 0.1125 - val_loss: 0.4604 - val_accuracy: 0.0480\n",
            "Epoch 52/126\n",
            "308/313 [============================>.] - ETA: 0s - loss: 0.3790 - accuracy: 0.1124\n",
            "Epoch 52: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3789 - accuracy: 0.1125 - val_loss: 0.4604 - val_accuracy: 0.0480\n",
            "Epoch 53/126\n",
            "310/313 [============================>.] - ETA: 0s - loss: 0.3790 - accuracy: 0.1122\n",
            "Epoch 53: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3789 - accuracy: 0.1125 - val_loss: 0.4609 - val_accuracy: 0.0480\n",
            "Epoch 54/126\n",
            "308/313 [============================>.] - ETA: 0s - loss: 0.3788 - accuracy: 0.1124\n",
            "Epoch 54: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3789 - accuracy: 0.1125 - val_loss: 0.4597 - val_accuracy: 0.0480\n",
            "Epoch 55/126\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.3789 - accuracy: 0.1125\n",
            "Epoch 55: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3789 - accuracy: 0.1125 - val_loss: 0.4605 - val_accuracy: 0.0480\n",
            "Epoch 56/126\n",
            "308/313 [============================>.] - ETA: 0s - loss: 0.3788 - accuracy: 0.1121\n",
            "Epoch 56: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.3789 - accuracy: 0.1125 - val_loss: 0.4608 - val_accuracy: 0.0480\n",
            "Epoch 57/126\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.3789 - accuracy: 0.1125\n",
            "Epoch 57: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3789 - accuracy: 0.1125 - val_loss: 0.4615 - val_accuracy: 0.0480\n",
            "Epoch 58/126\n",
            "310/313 [============================>.] - ETA: 0s - loss: 0.3790 - accuracy: 0.1124\n",
            "Epoch 58: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3789 - accuracy: 0.1125 - val_loss: 0.4614 - val_accuracy: 0.0480\n",
            "Epoch 59/126\n",
            "310/313 [============================>.] - ETA: 0s - loss: 0.3789 - accuracy: 0.1125\n",
            "Epoch 59: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3788 - accuracy: 0.1125 - val_loss: 0.4609 - val_accuracy: 0.0480\n",
            "Epoch 60/126\n",
            "310/313 [============================>.] - ETA: 0s - loss: 0.3789 - accuracy: 0.1132\n",
            "Epoch 60: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3789 - accuracy: 0.1125 - val_loss: 0.4600 - val_accuracy: 0.0480\n",
            "Epoch 61/126\n",
            "310/313 [============================>.] - ETA: 0s - loss: 0.3789 - accuracy: 0.1125\n",
            "Epoch 61: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3789 - accuracy: 0.1125 - val_loss: 0.4606 - val_accuracy: 0.0480\n",
            "Epoch 62/126\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.3788 - accuracy: 0.1126\n",
            "Epoch 62: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3788 - accuracy: 0.1125 - val_loss: 0.4607 - val_accuracy: 0.0480\n",
            "Epoch 63/126\n",
            "309/313 [============================>.] - ETA: 0s - loss: 0.3788 - accuracy: 0.1127\n",
            "Epoch 63: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3788 - accuracy: 0.1125 - val_loss: 0.4601 - val_accuracy: 0.0480\n",
            "Epoch 64/126\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.3788 - accuracy: 0.1125\n",
            "Epoch 64: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 4s 13ms/step - loss: 0.3788 - accuracy: 0.1125 - val_loss: 0.4611 - val_accuracy: 0.0480\n",
            "Epoch 65/126\n",
            "310/313 [============================>.] - ETA: 0s - loss: 0.3788 - accuracy: 0.1125\n",
            "Epoch 65: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.3788 - accuracy: 0.1125 - val_loss: 0.4614 - val_accuracy: 0.0480\n",
            "Epoch 66/126\n",
            "312/313 [============================>.] - ETA: 0s - loss: 0.3787 - accuracy: 0.1127\n",
            "Epoch 66: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3788 - accuracy: 0.1125 - val_loss: 0.4606 - val_accuracy: 0.0480\n",
            "Epoch 67/126\n",
            "312/313 [============================>.] - ETA: 0s - loss: 0.3788 - accuracy: 0.1126\n",
            "Epoch 67: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3788 - accuracy: 0.1125 - val_loss: 0.4609 - val_accuracy: 0.0480\n",
            "Epoch 68/126\n",
            "312/313 [============================>.] - ETA: 0s - loss: 0.3788 - accuracy: 0.1126\n",
            "Epoch 68: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3788 - accuracy: 0.1125 - val_loss: 0.4604 - val_accuracy: 0.0480\n",
            "Epoch 69/126\n",
            "309/313 [============================>.] - ETA: 0s - loss: 0.3788 - accuracy: 0.1128\n",
            "Epoch 69: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3788 - accuracy: 0.1125 - val_loss: 0.4601 - val_accuracy: 0.0480\n",
            "Epoch 70/126\n",
            "309/313 [============================>.] - ETA: 0s - loss: 0.3788 - accuracy: 0.1126\n",
            "Epoch 70: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4601 - val_accuracy: 0.0480\n",
            "Epoch 71/126\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.3788 - accuracy: 0.1125\n",
            "Epoch 71: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3788 - accuracy: 0.1125 - val_loss: 0.4604 - val_accuracy: 0.0480\n",
            "Epoch 72/126\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.3788 - accuracy: 0.1126\n",
            "Epoch 72: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.3788 - accuracy: 0.1125 - val_loss: 0.4608 - val_accuracy: 0.0480\n",
            "Epoch 73/126\n",
            "312/313 [============================>.] - ETA: 0s - loss: 0.3787 - accuracy: 0.1124\n",
            "Epoch 73: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3788 - accuracy: 0.1125 - val_loss: 0.4600 - val_accuracy: 0.0480\n",
            "Epoch 74/126\n",
            "312/313 [============================>.] - ETA: 0s - loss: 0.3788 - accuracy: 0.1125\n",
            "Epoch 74: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3788 - accuracy: 0.1125 - val_loss: 0.4612 - val_accuracy: 0.0480\n",
            "Epoch 75/126\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.3788 - accuracy: 0.1125\n",
            "Epoch 75: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.3788 - accuracy: 0.1125 - val_loss: 0.4600 - val_accuracy: 0.0480\n",
            "Epoch 76/126\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.3788 - accuracy: 0.1125\n",
            "Epoch 76: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3788 - accuracy: 0.1125 - val_loss: 0.4604 - val_accuracy: 0.0480\n",
            "Epoch 77/126\n",
            "309/313 [============================>.] - ETA: 0s - loss: 0.3788 - accuracy: 0.1131\n",
            "Epoch 77: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3788 - accuracy: 0.1125 - val_loss: 0.4611 - val_accuracy: 0.0480\n",
            "Epoch 78/126\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.3787 - accuracy: 0.1125\n",
            "Epoch 78: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4610 - val_accuracy: 0.0480\n",
            "Epoch 79/126\n",
            "312/313 [============================>.] - ETA: 0s - loss: 0.3787 - accuracy: 0.1127\n",
            "Epoch 79: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 4s 13ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4603 - val_accuracy: 0.0480\n",
            "Epoch 80/126\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.3788 - accuracy: 0.1125\n",
            "Epoch 80: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3788 - accuracy: 0.1125 - val_loss: 0.4608 - val_accuracy: 0.0480\n",
            "Epoch 81/126\n",
            "309/313 [============================>.] - ETA: 0s - loss: 0.3788 - accuracy: 0.1129\n",
            "Epoch 81: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4607 - val_accuracy: 0.0480\n",
            "Epoch 82/126\n",
            "310/313 [============================>.] - ETA: 0s - loss: 0.3787 - accuracy: 0.1126\n",
            "Epoch 82: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3788 - accuracy: 0.1125 - val_loss: 0.4602 - val_accuracy: 0.0480\n",
            "Epoch 83/126\n",
            "309/313 [============================>.] - ETA: 0s - loss: 0.3788 - accuracy: 0.1124\n",
            "Epoch 83: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4608 - val_accuracy: 0.0480\n",
            "Epoch 84/126\n",
            "310/313 [============================>.] - ETA: 0s - loss: 0.3788 - accuracy: 0.1128\n",
            "Epoch 84: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3788 - accuracy: 0.1125 - val_loss: 0.4605 - val_accuracy: 0.0480\n",
            "Epoch 85/126\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.3788 - accuracy: 0.1125\n",
            "Epoch 85: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3788 - accuracy: 0.1125 - val_loss: 0.4611 - val_accuracy: 0.0480\n",
            "Epoch 86/126\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.3789 - accuracy: 0.1124\n",
            "Epoch 86: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3788 - accuracy: 0.1125 - val_loss: 0.4602 - val_accuracy: 0.0480\n",
            "Epoch 87/126\n",
            "312/313 [============================>.] - ETA: 0s - loss: 0.3787 - accuracy: 0.1127\n",
            "Epoch 87: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4609 - val_accuracy: 0.0480\n",
            "Epoch 88/126\n",
            "310/313 [============================>.] - ETA: 0s - loss: 0.3786 - accuracy: 0.1127\n",
            "Epoch 88: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4611 - val_accuracy: 0.0480\n",
            "Epoch 89/126\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.3787 - accuracy: 0.1125\n",
            "Epoch 89: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4607 - val_accuracy: 0.0480\n",
            "Epoch 90/126\n",
            "312/313 [============================>.] - ETA: 0s - loss: 0.3787 - accuracy: 0.1124\n",
            "Epoch 90: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4602 - val_accuracy: 0.0480\n",
            "Epoch 91/126\n",
            "312/313 [============================>.] - ETA: 0s - loss: 0.3787 - accuracy: 0.1126\n",
            "Epoch 91: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 4s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4611 - val_accuracy: 0.0480\n",
            "Epoch 92/126\n",
            "312/313 [============================>.] - ETA: 0s - loss: 0.3787 - accuracy: 0.1127\n",
            "Epoch 92: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 4s 13ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4610 - val_accuracy: 0.0480\n",
            "Epoch 93/126\n",
            "312/313 [============================>.] - ETA: 0s - loss: 0.3786 - accuracy: 0.1125\n",
            "Epoch 93: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 5s 16ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4606 - val_accuracy: 0.0480\n",
            "Epoch 94/126\n",
            "312/313 [============================>.] - ETA: 0s - loss: 0.3787 - accuracy: 0.1127\n",
            "Epoch 94: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4603 - val_accuracy: 0.0480\n",
            "Epoch 95/126\n",
            "310/313 [============================>.] - ETA: 0s - loss: 0.3787 - accuracy: 0.1124\n",
            "Epoch 95: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4604 - val_accuracy: 0.0480\n",
            "Epoch 96/126\n",
            "312/313 [============================>.] - ETA: 0s - loss: 0.3788 - accuracy: 0.1123\n",
            "Epoch 96: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4605 - val_accuracy: 0.0480\n",
            "Epoch 97/126\n",
            "308/313 [============================>.] - ETA: 0s - loss: 0.3787 - accuracy: 0.1124\n",
            "Epoch 97: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4605 - val_accuracy: 0.0480\n",
            "Epoch 98/126\n",
            "312/313 [============================>.] - ETA: 0s - loss: 0.3788 - accuracy: 0.1125\n",
            "Epoch 98: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4606 - val_accuracy: 0.0480\n",
            "Epoch 99/126\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.3787 - accuracy: 0.1125\n",
            "Epoch 99: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4605 - val_accuracy: 0.0480\n",
            "Epoch 100/126\n",
            "309/313 [============================>.] - ETA: 0s - loss: 0.3789 - accuracy: 0.1129\n",
            "Epoch 100: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4608 - val_accuracy: 0.0480\n",
            "Epoch 101/126\n",
            "309/313 [============================>.] - ETA: 0s - loss: 0.3787 - accuracy: 0.1125\n",
            "Epoch 101: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4606 - val_accuracy: 0.0480\n",
            "Epoch 102/126\n",
            "310/313 [============================>.] - ETA: 0s - loss: 0.3785 - accuracy: 0.1129\n",
            "Epoch 102: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4606 - val_accuracy: 0.0480\n",
            "Epoch 103/126\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.3788 - accuracy: 0.1126\n",
            "Epoch 103: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4607 - val_accuracy: 0.0480\n",
            "Epoch 104/126\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.3787 - accuracy: 0.1126\n",
            "Epoch 104: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4608 - val_accuracy: 0.0480\n",
            "Epoch 105/126\n",
            "312/313 [============================>.] - ETA: 0s - loss: 0.3787 - accuracy: 0.1127\n",
            "Epoch 105: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4609 - val_accuracy: 0.0480\n",
            "Epoch 106/126\n",
            "312/313 [============================>.] - ETA: 0s - loss: 0.3787 - accuracy: 0.1126\n",
            "Epoch 106: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4610 - val_accuracy: 0.0480\n",
            "Epoch 107/126\n",
            "308/313 [============================>.] - ETA: 0s - loss: 0.3784 - accuracy: 0.1131\n",
            "Epoch 107: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4610 - val_accuracy: 0.0480\n",
            "Epoch 108/126\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.3787 - accuracy: 0.1125\n",
            "Epoch 108: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4612 - val_accuracy: 0.0480\n",
            "Epoch 109/126\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.3787 - accuracy: 0.1125\n",
            "Epoch 109: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4609 - val_accuracy: 0.0480\n",
            "Epoch 110/126\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.3787 - accuracy: 0.1122\n",
            "Epoch 110: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4608 - val_accuracy: 0.0480\n",
            "Epoch 111/126\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.3787 - accuracy: 0.1125\n",
            "Epoch 111: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4610 - val_accuracy: 0.0480\n",
            "Epoch 112/126\n",
            "312/313 [============================>.] - ETA: 0s - loss: 0.3787 - accuracy: 0.1125\n",
            "Epoch 112: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4611 - val_accuracy: 0.0480\n",
            "Epoch 113/126\n",
            "308/313 [============================>.] - ETA: 0s - loss: 0.3787 - accuracy: 0.1123\n",
            "Epoch 113: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4610 - val_accuracy: 0.0480\n",
            "Epoch 114/126\n",
            "313/313 [==============================] - ETA: 0s - loss: 0.3787 - accuracy: 0.1125\n",
            "Epoch 114: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4609 - val_accuracy: 0.0480\n",
            "Epoch 115/126\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.3786 - accuracy: 0.1127\n",
            "Epoch 115: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4611 - val_accuracy: 0.0480\n",
            "Epoch 116/126\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.3787 - accuracy: 0.1124\n",
            "Epoch 116: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4611 - val_accuracy: 0.0480\n",
            "Epoch 117/126\n",
            "308/313 [============================>.] - ETA: 0s - loss: 0.3787 - accuracy: 0.1130\n",
            "Epoch 117: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4611 - val_accuracy: 0.0480\n",
            "Epoch 118/126\n",
            "312/313 [============================>.] - ETA: 0s - loss: 0.3787 - accuracy: 0.1125\n",
            "Epoch 118: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4611 - val_accuracy: 0.0480\n",
            "Epoch 119/126\n",
            "312/313 [============================>.] - ETA: 0s - loss: 0.3786 - accuracy: 0.1125\n",
            "Epoch 119: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4612 - val_accuracy: 0.0480\n",
            "Epoch 120/126\n",
            "312/313 [============================>.] - ETA: 0s - loss: 0.3787 - accuracy: 0.1123\n",
            "Epoch 120: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4611 - val_accuracy: 0.0480\n",
            "Epoch 121/126\n",
            "312/313 [============================>.] - ETA: 0s - loss: 0.3787 - accuracy: 0.1126\n",
            "Epoch 121: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 4s 13ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4611 - val_accuracy: 0.0480\n",
            "Epoch 122/126\n",
            "309/313 [============================>.] - ETA: 0s - loss: 0.3788 - accuracy: 0.1128\n",
            "Epoch 122: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4610 - val_accuracy: 0.0480\n",
            "Epoch 123/126\n",
            "309/313 [============================>.] - ETA: 0s - loss: 0.3788 - accuracy: 0.1114\n",
            "Epoch 123: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 4s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4611 - val_accuracy: 0.0480\n",
            "Epoch 124/126\n",
            "309/313 [============================>.] - ETA: 0s - loss: 0.3785 - accuracy: 0.1129\n",
            "Epoch 124: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4611 - val_accuracy: 0.0480\n",
            "Epoch 125/126\n",
            "311/313 [============================>.] - ETA: 0s - loss: 0.3787 - accuracy: 0.1127\n",
            "Epoch 125: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 4s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4612 - val_accuracy: 0.0480\n",
            "Epoch 126/126\n",
            "308/313 [============================>.] - ETA: 0s - loss: 0.3786 - accuracy: 0.1124\n",
            "Epoch 126: val_accuracy did not improve from 0.04800\n",
            "313/313 [==============================] - 4s 11ms/step - loss: 0.3787 - accuracy: 0.1125 - val_loss: 0.4612 - val_accuracy: 0.0480\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "MRnK4j_PxmML",
        "outputId": "ae4b7af6-70f7-4b82-b3f4-a747cb919c32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZhcdZ3v8fe3ll7T6SzdBLKQBAgYNhPSRBBBFIEAGnBwEBFHHAXmjlwdx2GGXLcrzr2jd3wQHRFFzcioAzJBNEocwq4iSzoQA0kIaUJCOgmkyZ50eqmq7/3jd6q7ulNJupOurl4+r+fpp6vOVt86p+p8zu9sZe6OiIhId7FiFyAiIgOTAkJERPJSQIiISF4KCBERyUsBISIieSkgREQkLwWESB8ws5+Y2T/3cNh1Zva+I52OSKEpIEREJC8FhIiI5KWAkGEj2rVzs5ktN7O9ZvZjMxtnZr8zs91m9oiZjc4Zfq6ZrTCzHWb2hJlNz+k308yej8b7BVDW7bXeb2bLonH/ZGanH2bN15tZg5ltM7OFZjY+6m5m9i0z22Jmu8zsRTM7Nep3qZmtjGrbaGb/cFgzTIY9BYQMN1cCFwInAh8Afgf8L6CW8H34DICZnQjcA/xd1G8R8BszKzGzEuBXwE+BMcB/RdMlGncmMB+4ERgL/ABYaGalvSnUzN4L/AtwFXAMsB64N+p9EXBe9D6qo2G2Rv1+DNzo7lXAqcBjvXldkSwFhAw3/+bub7r7RuAPwLPu/oK7twAPADOj4T4MPOjuD7t7O/BNoBx4J3AWkARud/d2d18ALMl5jRuAH7j7s+6edve7gdZovN74KDDf3Z9391ZgHnC2mU0B2oEq4G2Aufsqd98cjdcOnGxmI919u7s/38vXFQEUEDL8vJnzeF+e5yOix+MJW+wAuHsG2ABMiPpt9K53ulyf83gy8Plo99IOM9sBTIrG643uNewhtBImuPtjwHeBO4AtZnaXmY2MBr0SuBRYb2ZPmtnZvXxdEUABIXIgmwgreiDs8yes5DcCm4EJUbesY3MebwD+j7uPyvmrcPd7jrCGSsIuq40A7v4dd58FnEzY1XRz1H2Ju18OHEXYFXZfL19XBFBAiBzIfcBlZnaBmSWBzxN2E/0JeBpIAZ8xs6SZ/QUwO2fcHwJ/Y2bviA4mV5rZZWZW1csa7gE+YWYzouMX/5ewS2ydmZ0ZTT8J7AVagEx0jOSjZlYd7RrbBWSOYD7IMKaAEMnD3VcD1wL/BrxFOKD9AXdvc/c24C+A64BthOMVv8wZtx64nrALaDvQEA3b2xoeAb4E3E9otRwPXB31HkkIou2E3VBbgX+N+n0MWGdmu4C/IRzLEOk10w8GiYhIPmpBiIhIXgoIERHJSwEhIiJ5KSBERCSvRLEL6Cs1NTU+ZcqUYpchIjKoLF269C13r83Xb8gExJQpU6ivry92GSIig4qZrT9QP+1iEhGRvBQQIiKSlwJCRETyGjLHIPJpb2+nsbGRlpaWYpdScGVlZUycOJFkMlnsUkRkiChoQJjZHODbQBz4kbt/Pc8wVwH/G3Dgz+5+TdT948AXo8H+Obqnfq80NjZSVVXFlClT6HrjzaHF3dm6dSuNjY1MnTq12OWIyBBRsIAwszjhXvUXAo3AEjNb6O4rc4aZRvgRlHPcfbuZHRV1HwN8BagjBMfSaNztvamhpaVlyIcDgJkxduxYmpqail2KiAwhhTwGMRtocPe10d0v7wUu7zbM9cAd2RW/u2+Jul8MPOzu26J+DwNzDqeIoR4OWcPlfYpI/ylkQEwg/HBKVmPULdeJwIlm9pSZPRPtkurpuJjZDWZWb2b1h7v1nM44b+xsobktdVjji4gMVcU+iykBTAPOBz4C/NDMRvV0ZHe/y93r3L2utjbvhYA9mQZbdrfQ3JY+rPEPZceOHXzve9/r9XiXXnopO3bsKEBFIiI9U8iA2Ej4icasiVG3XI3AwuiH318DXiEERk/G7VsF+lmMAwVEKnXwFsuiRYsYNarHWSki0ucKGRBLgGlmNtXMSgi/hLWw2zC/IrQeMLMawi6ntcBDwEVmNtrMRgMXRd36XHbffaF+NumWW27h1VdfZcaMGZx55pmce+65zJ07l5NPPhmAK664glmzZnHKKadw1113dYw3ZcoU3nrrLdatW8f06dO5/vrrOeWUU7jooovYt29fgaoVEelUsLOY3D1lZjcRVuxxYL67rzCzW4F6d19IZxCsBNLAze6+FcDMvkYIGYBb3X3bkdTz1d+sYOWmXXn77W1NUZKIkYz3Li9PHj+Sr3zglIMO8/Wvf52XXnqJZcuW8cQTT3DZZZfx0ksvdZyOOn/+fMaMGcO+ffs488wzufLKKxk7dmyXaaxZs4Z77rmHH/7wh1x11VXcf//9XHvttb2qVUSktwp6HYS7LwIWdev25ZzHDvx99Nd93PnA/ELWVwyzZ8/ucq3Cd77zHR544AEANmzYwJo1a/YLiKlTpzJjxgwAZs2axbp16/qtXhEZvob0ldS5DrSl7+68uHEn40aWMW5kWcHrqKys7Hj8xBNP8Mgjj/D0009TUVHB+eefn/eq79LS0o7H8Xhcu5hEpF8U+yymous4BlGggxBVVVXs3r07b7+dO3cyevRoKioqePnll3nmmWcKU4SIyGEYNi2IgwkhUZiEGDt2LOeccw6nnnoq5eXljBs3rqPfnDlz+P73v8/06dM56aSTOOusswpSg4jI4TAv1KZzP6urq/PuPxi0atUqpk+ffshxX9q4k7EjSjimurxQ5fWLnr5fEZEsM1vq7nX5+g37XUxZQyQnRUT6jAIC0G2MRET2p4AADFMLQkSkGwUEgIEX7FpqEZHBSQEBGBTuXhsiIoOUAoIQEMoHEZGuFBAQ7WIqjMO93TfA7bffTnNzcx9XJCLSMwoIwkHqQh2lVkCIyGClK6kJp7n2x+2+L7zwQo466ijuu+8+Wltb+eAHP8hXv/pV9u7dy1VXXUVjYyPpdJovfelLvPnmm2zatIn3vOc91NTU8PjjjxeoQhGR/IZPQPzuFnjjxby9JraniGGQjPdumkefBpd8/aCD5N7ue/HixSxYsIDnnnsOd2fu3Ln8/ve/p6mpifHjx/Pggw8C4R5N1dXV3HbbbTz++OPU1NT0ri4RkT6gXUz9aPHixSxevJiZM2dyxhln8PLLL7NmzRpOO+00Hn74Yf7pn/6JP/zhD1RXVxe7VBGRYdSCOMiW/qYte4jHjKk1lQccpi+4O/PmzePGG2/cr9/zzz/PokWL+OIXv8gFF1zAl7/85TxTEBHpP2pBRAp108Lc231ffPHFzJ8/nz179gCwceNGtmzZwqZNm6ioqODaa6/l5ptv5vnnn99vXBGR/jZ8WhAHUciD1Lm3+77kkku45pprOPvsswEYMWIEP/vZz2hoaODmm28mFouRTCa58847AbjhhhuYM2cO48eP10FqEel3ut03sLZpD+5w/FEjClVev9DtvkWkt3S77x4YGjEpItJ3FBAU9hflREQGqyEfED3ZhWYM/h8MGiq7CkVk4BjSAVFWVsbWrVt7tPIczKtXd2fr1q2UlZUVuxQRGUKG9FlMEydOpLGxkaampoMOt3VvG6l0hvS2wbuCLSsrY+LEicUuQ0SGkCEdEMlkkqlTpx5yuP95zwus2LiTx/5hZj9UJSIyOAzpXUw9lYgZqcxg3skkItL3FBBAPGakFRAiIl0oIIC4GalMpthliIgMKAoIIB5XC0JEpDsFBOEYhAJCRKQrBQThGIQOUouIdKWAQC0IEZF8FBBATC0IEZH9KCBQC0JEJB8FBBCPxUhnXDe8ExHJoYAgtCAA1IgQEelU0IAwszlmttrMGszsljz9rzOzJjNbFv19KqdfOqf7wkLWGY8CQhfLiYh0KtjN+swsDtwBXAg0AkvMbKG7r+w26C/c/aY8k9jn7jMKVV+ubEDoOISISKdCtiBmAw3uvtbd24B7gcsL+HqHLaGAEBHZTyEDYgKwIed5Y9StuyvNbLmZLTCzSTndy8ys3syeMbMr8r2Amd0QDVN/qN98OBi1IERE9lfsg9S/Aaa4++nAw8DdOf0mu3sdcA1wu5kd331kd7/L3evcva62tvawi0h0HINQQIiIZBUyIDYCuS2CiVG3Du6+1d1bo6c/Ambl9NsY/V8LPAEU7Nd84rEwG9SCEBHpVMiAWAJMM7OpZlYCXA10ORvJzI7JeToXWBV1H21mpdHjGuAcoPvB7T4Tj+aCWhAiIp0KdhaTu6fM7CbgISAOzHf3FWZ2K1Dv7guBz5jZXCAFbAOui0afDvzAzDKEEPt6nrOf+ky2BZFRQIiIdCjob1K7+yJgUbduX855PA+Yl2e8PwGnFbK2XDoGISKyv2IfpB4QOs9i0oVyIiJZCghyr6RWC0JEJEsBQU5ApBUQIiJZCghyb9angBARyVJAoF1MIiL5KCCAhC6UExHZjwICiGUvlNMxCBGRDgoI1IIQEclHAUHOdRA6SC0i0kEBQe7vQehCORGRLAUEug5CRCQfBQT6wSARkXwUEOhmfSIi+Sgg6GxB6EpqEZFOCgg6T3PVMQgRkU4KCCAe1zEIEZHuFBBA3HQMQkSkOwUEulBORCQfBQQ5F8qldaGciEiWAoLOYxDaxSQi0kkBQecxCB2kFhHppIBAPxgkIpKPAoKcnxxVQIiIdFBAoBaEiEg+CgjAzIjHTMcgRERyKCAicTO1IEREciggIqEFoesgRESyFBCRRMzQdXIiIp0UEJF4XC0IEZFcCohIIqZjECIiuRQQkZjpLCYRkVwKiIhaECIiXSkgIvG46UpqEZEcCohIIhZTC0JEJIcCIqIrqUVEulJARMKV1DrNVUQkq6ABYWZzzGy1mTWY2S15+l9nZk1mtiz6+1ROv4+b2Zro7+OFrBPUghAR6S5RqAmbWRy4A7gQaASWmNlCd1/ZbdBfuPtN3cYdA3wFqAMcWBqNu71Q9SbiCggRkVyFbEHMBhrcfa27twH3Apf3cNyLgYfdfVsUCg8DcwpUJxBaEDpILSLSqZABMQHYkPO8MerW3ZVmttzMFpjZpN6Ma2Y3mFm9mdU3NTUdUbFxXSgnItJFsQ9S/waY4u6nE1oJd/dmZHe/y93r3L2utrb2iApRC0JEpKtCBsRGYFLO84lRtw7uvtXdW6OnPwJm9XTcvpbQhXIiIl0UMiCWANPMbKqZlQBXAwtzBzCzY3KezgVWRY8fAi4ys9FmNhq4KOpWMHFdKCci0kWPAsLMPmtmIy34sZk9b2YXHWwcd08BNxFW7KuA+9x9hZndamZzo8E+Y2YrzOzPwGeA66JxtwFfI4TMEuDWqFvBJHSaq4hIFz09zfWv3f3bZnYxMBr4GPBTYPHBRnL3RcCibt2+nPN4HjDvAOPOB+b3sL4jFtNPjoqIdNHTXUwW/b8U+Km7r8jpNiQk9JOjIiJd9DQglprZYkJAPGRmVcCQWpvGdaGciEgXPd3F9ElgBrDW3ZujK50/Ubiy+p+OQYiIdNXTFsTZwGp332Fm1wJfBHYWrqz+p+sgRES66mlA3Ak0m9nbgc8DrwL/UbCqikBXUouIdNXTgEi5uxPupfRdd78DqCpcWf0vEVcLQkQkV0+PQew2s3mE01vPNbMYkCxcWf0vHtOV1CIiuXragvgw0Eq4HuINwq0v/rVgVRWBfnJURKSrHgVEFAo/B6rN7P1Ai7sPrWMQOotJRKSLnt5q4yrgOeAvgauAZ83sQ4UsrL+Fs5iG1KUdIiJHpKfHIL4AnOnuWwDMrBZ4BFhQqML6m1oQIiJd9fQYRCwbDpGtvRh3UNCFciIiXfW0BfHfZvYQcE/0/MN0uwnfYBePGRmHTMaJxYbUbaZERA5LjwLC3W82syuBc6JOd7n7A4Urq//FLYRC2p3Y0LoPoYjIYelpCwJ3vx+4v4C1FFU8HgVExknGi1yMiMgAcNCAMLPdQL4d8wa4u48sSFVFkIh2K+laCBGR4KAB4e5D6nYaBxOPhWPuOlAtIhIMqTORjkS2BaGAEBEJFBCRWMcuJl0sJyICCogOakGIiHSlgIjEFRAiIl0oICJqQYiIdKWAiMR1mquISBcKiIh2MYmIdKWAiHRcKJdWQIiIgAKiQ/ZCuYwrIEREQAHRQbfaEBHpqsc36xvqOo9BDOML5dxh71uwYz2MOQ4qxhS7oqBtL+zcCLUn9v9rt7fAxqWw5w2IJSFZATXTYNSxYL286297C+zaCM3b4OhTIVnet7W6976mQki3Qzx5gH6p8D8+QFc96RS07ARPQ2Vt38xPd8ikor90tmN47BmIJcLnKp6ATAYy7aFbLOeuoen28D8WzbdUC6Ra6bhVXiwBpX1/Z6QBupT6X/xIj0FkMhDr4wZZqg1eWgD7doTn5aNh3ClQexIkSvPUkIYtq6DqGKgc2/PX2b4e/vRvsPw+aN0ZupVUwdl/C2f9LZSP6hy2fR9seA7eWA5NL0cfXIOyahh7Qqht8jk9WwHsbIQ/fit8EY89C6rGQ/NbsG97+OJ4BtY+AS8ugNZdMH0uXPrNsPJ54aew9klo3Q3tzTB6ChwzI3xJ3nwpTPuMv4JTPhi+5Ls2w5rF4YtvsdB/y6rwfmZ+NEw71QINj8CmF8LwOzfAxuch3bp/7aUjYcq5cPpVMOVdYX40LoV928J0khUwfmaYH68+Hubtmy92jp8og6nnQdXRsPsNaN0DNSfAuFND+FTWhuUdT4LFYe+WUPP29bDt1TDOhFlw0iWh+9PfhXVPwfT3w+wbYdQk2LYW3ngRXvs9rH8aUvsgXhKmPfkcmHQm7N0Kb60O8zyWCHWNOyXU3rYHGuth+zoYMS7Uum0tbHg2zPeTLoW3XQY7Xod1fwjzc9dmaNsdPoM10+Cok8NyKR8NK38Nq34T+sdLQ7cxx8HY48PyGzU5dEu1hL/25rB80m1hJeuZ8DjVGh5bLDzfsyXMn2QFVNZAyYjQPd0WVviZVDS9fWGaqdawTNujbqmWMGwmFd5zVrICqieFII/FQ//WPWGY7LTcwzKKl4ThkhXhO9G6G9r3hnF6yuLh85n7+vGSsIGUaT/4uBPq4PpHe/5aPS3Jh8g+97q6Oq+vrz+8kfc08cybxtU/fJb//NQ7eOcJNb0bf+urMH8OTD0XLrut6wo1q30fLL0bVi8KK8PpHwhbxct+BpuXhy/stAth0mwYOSF8sX/1P8LKrrtEGcz6BJz7eSgdAS8/GL58r/0eWnaED9qUd4XX2fsW7G0KX9QTLggr8b1vhZXfphegcQmseTh82U79Cxh/BowcDy/dDyt/FVaEp38Y3n41rH0cnrkTmreGOiqPiraCPWwVZ79cVeNh1sfDl377eti9KfpStsNR02HaRbB1Dfz2c9EKoJ38Nw0GEuVwyhVQPRGe+k547+nW8KUed2pY2SXKYGtDmCZA+ZgwX3a8Hq2Eo/eT+yWzeFg5pdtCi6myNmw5ptvCyqvq6DAfJswK83L01DB+6+4QjJuXw+rfhZZF50RDQCVKw4okta+z14RZMO3isPIvHQHr/hjme9ue8FrJCmhaHQLmUMpGhXqz7zc7z094L6z6bfgM5Bo7LXw2y0eH5bBjfQiT7GtVTworVs+Euret7Vwe8dJQcza4K2th0jvCiuuVh8JKEGDkRJgwM3x2y0aFz1fTatiyMqxIIXyWps+F0ZPD+967NbzW1oawgu8pi4U/97DiHjEu1JVqCZ/1tr2hvnhJWHnHEuFxSUX4PCXLwvtKloe/RGnoH0tC2cgwnyB8fnasD/Mskw6vVToyLL9kRfjcmYXPb7o9zIu25jC90qowTCwRao0nOh9nf28mFg+fw0wqCq6WUEM8GYXR7vB5LKmEZGUYLZ0KyyZRFl7Hoo3Sylo47UM9n4e5s9NsqbvX5e037APirQb40QWsf/vnePeT0/jpJ2dz7rTa0C/VBq/8LiyEaRdDoiR8KN9cEVYeFWPCB+LHF4YVYXtz+IKc+7mwVbPj9fDBMoOGR8OXYMxxYYvMo11ZlbUw8Ux4/enwBQQorQ4ftoqx8P5vhfDAYU9T2ApteBT+fE/4AFo8bPVnVxCT3xW+cCseCFua5aPD37bXyLsSHntCeG9nfxqqJ3Ttt3l5aFms/HXnVvS0i+DM62HCGWGlkuUOe94MrYulP4FXs1szFoZLlIf5sGN95zgT6uDKH4b32bgkrDBG1IYVfCwepjl6cmidADS9Ao99Lcz32TeELd1cLTtD4IwYF+Zv/fwwfDoFZ3wMZl0XVl6eiWoqDS2/NYth2c/DivCkS6MVYA9aQJl0aOFs/jMc8/awHMuiO+Cn28MW9ZaVIRxqph16etl5uGtjWNYtO8J0PA0VNWH5jJrcuetv9xuh9pIRYYMjngyfx+zyGnM81JwIVePy1J7pDMbSEd3m467QIkqWw7jTwucewhZ3orRzt0tbM2x4JmwIjJ6af3dMJg1vvQK7N8OxZx94t1pbc/i+tOwMK/BEWfh8JyuiVpRFK9rSgbt7apBSQBxMJg33XoOveZhrWm/hho9fx3sml8LT34Ol/x6+sBC2lo87H9Y/Fb7ApdVw3j+EsFj+C/jogtByuP+TIQCw0MyOJ8IXv+ZEOPfvw9boni1h66tibGg1xJOhjo3Pwxt/DtNMlMF5Nx/4OEDTK/DH28Ljt38k7O7I3cXlHlYu2S9387awMtu9OaxAR4wL+8GzW0sHs3drCMqjT4djTu/ZfN3ZGFbW1ZPCFz5r9xthyzmTgpnXHnhfdV9pawY8bIWJyH4UEIfSsouWH1xAy7aNbJlxEyc2zA+7YaZdBLOvDyvbpT8J4TDlXWGl/vKDYesN4Px5cP4t4XH7vrBy7L5iFBEZgBQQPfDKy8upvecSRtsemDgbLv1/4UDdwTQ8Gpri7/xs3x+gFhHpBwcLCO3Mi/ioqVzT9gW+dt4I6uZ8rGent51wQfgTERmCFBCReMxY5ZPZNH7mwDiXXESkyAq6X8TM5pjZajNrMLNbDjLclWbmZlYXPZ9iZvvMbFn09/1C1gm5t/sexhfKiYjkKFgLwsziwB3AhUAjsMTMFrr7ym7DVQGfBZ7tNolX3X1Goerr7ogvlBMRGWIK2YKYDTS4+1p3bwPuBS7PM9zXgG8ALQWs5ZB0u28Rka4KGRATgA05zxujbh3M7Axgkrs/mGf8qWb2gpk9aWbn5nsBM7vBzOrNrL6pqemIitXN+kREuirauZlmFgNuAz6fp/dm4Fh3nwn8PfCfZjay+0Dufpe717l7XW1t7RHVk21B6HbfIiJBIQNiIzAp5/nEqFtWFXAq8ISZrQPOAhaaWZ27t7r7VgB3Xwq8ChT0Vp6J6DoGHYMQEQkKGRBLgGlmNtXMSoCrgYXZnu6+091r3H2Ku08BngHmunu9mdVGB7kxs+OAacDaAtbacZ2bjkGIiAQFO4vJ3VNmdhPwEBAH5rv7CjO7Fah394UHGf084FYzawcywN+4ew9uc3n4OloQCggREaDAF8q5+yJgUbduXz7AsOfnPL4fuL+QtXWnYxAiIl3pBkKRhK6DEBHpQgERicUs/PaHrqQWEQEUEF3EzXQMQkQkooDIEY+ZzmISEYkoIHIkFBAiIh0UEDniMe1iEhHJUkDk0C4mEZFOCogc8VhMLQgRkYgCIkc4BqHTXEVEQAHRRdjFVOwqREQGBgVEjkRcLQgRkSwFRA5dKCci0kkBkUNnMYmIdFJA5NB1ECIinRQQORJxI6OAEBEBFBBd6DoIEZFOCogccdNPjoqIZCkgciRiMVI6zVVEBFBAdKGzmEREOikgcoQL5RQQIiKggOhCLQgRkU4KiBy6klpEpJMCIodaECIinRQQOXQMQkSkkwIiRyIWoyWVLnYZIiIDggIixwlHjaBx+z52tbQXuxQRkaJTQOSYeewo3GH5hp3FLkVEpOgUEDlOnzgKgGUbthe5EhGR4lNA5KguT3J8bSUvvL6j2KWIiBSdAqKbmceOZtmGHbjrbCYRGd4UEN3MmDSKrXvbaNy+r9iliIgUlQKim5nHhuMQz7+u4xAiMrwpILo5aVwV5ck4yzboOISIDG8KiG4S8RinTazWgWoRGfYUEHnMnDSKlZt20aqrqkVkGFNA5DFj0ija0hle2qgL5kRk+CpoQJjZHDNbbWYNZnbLQYa70szczOpyus2LxlttZhcXss7u3nl8DZUlcX7yp/X9+bIiIgNKwQLCzOLAHcAlwMnAR8zs5DzDVQGfBZ7N6XYycDVwCjAH+F40vX5RXZHkY2dP4bfLN/Fq057+elkRkQGlkC2I2UCDu6919zbgXuDyPMN9DfgG0JLT7XLgXndvdffXgIZoev3mU+dOpTQR43uPv9qfLysiMmAUMiAmABtynjdG3TqY2RnAJHd/sLfjRuPfYGb1Zlbf1NTUN1VHakaUcs3syfxq2UY2bGvu02mLiAwGRTtIbWYx4Dbg84c7DXe/y93r3L2utra274qL3Pju44ib8e1H1/T5tEVEBrpCBsRGYFLO84lRt6wq4FTgCTNbB5wFLIwOVB9q3H4xbmQZn3jXFBYsbeSJ1Vv6++VFRIqqkAGxBJhmZlPNrIRw0Hlhtqe773T3Gnef4u5TgGeAue5eHw13tZmVmtlUYBrwXAFrPaDPve9EThw3gn9csJwdzW3FKEFEpCgKFhDungJuAh4CVgH3ufsKM7vVzOYeYtwVwH3ASuC/gU+7e1GuWitLxrntqhls29vGF3/1UjFKEBEpChsqt7Wuq6vz+vr6gk3/u4+t4ZuLX2HeJW/jxncfX7DXERHpT2a21N3r8vVL9Hcxg9Xfnn8CL7+xm3/53cuMqSzhL+smHXokEZFBTAHRQ7GYcdtVM9i5r51bfvkiG3fs471vO4pTxlcTj1mxyxMR6XO6F1MvlCRi3HntLN55/Fhuf2QNc7/7FOd+4zHq120rdmkiIn1OAdFLI0oT/PST72DJF97H7R+eQUkixtV3PcO/P/WafqZURIYUBcRhqq0q5YqZE/j1Te/i/JNq+epvVnLFHU/x62UbaUtlil2eiMgR01lMfSCTce5dsoEf/WEta9/ay6iKJOccX8O502p490m1HFNdXpS6REQO5WBnMSkg+lAm4zz5ShO/XT3DUlIAAArxSURBVL6ZPzY08eauVgDednQV5590FOedWEPd5DGUJNRwE5GBQQFRBO7Omi17eGL1Fh57eQv167aTyjjlyTgnjx/JaROqecfUMZx3Yi2VpTqZTESKQwExAOxpTfH0q1t5quEtVmzayYpNu2huS1OaiDFj0igScSOdcSaPqWTGsaM4bUI1k8dWUFWWLHbpIjKEKSAGoFQ6w5J123loxRv8uXEHcTPMoGHLHrY3t3cMN6ayhEmjy5k4uoKjq8sYXZGkuqKE6vIk1eVJJowqZ2pNpa7FEJHDoiupB6BEPMbZx4/l7OPHdunu7qzb2syqzbt4fVsz67c207i9mZWbd/HE6i3sbdv/llSliRiTx1ZgGGl3qsoSjKsqY+yIEsqScUoTMSpLE1SWxKksTTCiNEFlaYKyZJySRIyqsgQ1I0oZWZbATEEjIoECYoAxM6bWVDK1pjJv/9ZUmp3N7ezc186Ofe2szwmTmEHMjF0t7TQ07WHJujZa2tO0pDKkM4duKSbjRnV5CaMqkmQyzs597bS0pylLxilLxqkqSzCyPEl5Mk52arUjSpkwqozSZJyd+9rZta+dVMZJZ5yyZIyR5UlGliVJxo1ELIYZuIMZJOMxSuIxKkrjjChNUJKI4Q7pjOOEsEzEYpQlYx1hlozHSMSMZDxGPGZkPLxWaypDS3saMxhZlmREWYK4Wcd0Mg44YBAziMcs/JkRi1pvueHo7mQb19nO2Vmo1poMFwqIQaY0EeeokXGOGlkGwJlTxhxyHHenLZ1hb2uava0p9rSmaG5L0dqeoSWVZndLiqbdrWzd28aO5jZ2NLcTjxnV5UnKknFaU2ma28Jwu/a1s6O5DTMLB+Lf3M2bu1rIOCEQypIk4zFiMdjXlmHXvnba0oPnupBYtzDIJx4zSqMz0drTGVI5A8ctBE8i1hk8DhCFYkkiBJthOCG4UunOALdsWNE1xLI1ZaKwc3fMwuuEft4R2tn4yn0eMwsbENE0Y2YYkMo4qXSGjHcGXzZ0Y9F7icUgkyGnxs7XyNYMEIvRMd0QzF3nW3a8fK1Uyynco/eZKzvd7MSzwe857y/fRLvPi9zXy9ZxoN3s+bpaD/ofarrdN0QONUz3d5ZvjLcdXcV3rzkj77SOhAJiGDAzShNxShNxxlSW9Pn026MVXFkyvl8/97B1n8o47dEFhNlWRHsmQ3va2duaYndLO62pTNiij1m0og4H7lva07S0p2lPO23p8D+VdtLuoTVgRkkiRnkyTsZhd0s7u1tSZFeZYUXdueLKZJyMe0dLJ7tC6ljpODk1dK7o4jELdac7WyvZlkzHtD3UlsqZbpf3nM7Qns4Qre6IRWGSXTl7Rx2QdieTCdOyjvcRxjFCYKQzYVqxWJhf4B0ttM5l0BksmUyYb9luiZiRiMeIWWcAZUPOPQRIxrNh0Tm97IqZnPft3tla6x4Gue9rv88InStKh47jcdalf9fQMOu6fDJRYOZ+7vKFQnZ6B+yZW5QdIBBy3vx+/Xsy3e4OMsz+7yI7SteRJo+tyDvckVJAyBFLxmPkyQYgfJE7gqO0/2oSkSOnK7ZERCQvBYSIiOSlgBARkbwUECIikpcCQkRE8lJAiIhIXgoIERHJSwEhIiJ5DZm7uZpZE7D+CCZRA7zVR+UUw2CvH/QeBoLBXj/oPfTWZHevzddjyATEkTKz+gPd8nYwGOz1g97DQDDY6we9h76kXUwiIpKXAkJERPJSQHS6q9gFHKHBXj/oPQwEg71+0HvoMzoGISIieakFISIieSkgREQkr2EfEGY2x8xWm1mDmd1S7Hp6wswmmdnjZrbSzFaY2Wej7mPM7GEzWxP9H13sWg/GzOJm9oKZ/TZ6PtXMno2WxS/MrO9//q4PmdkoM1tgZi+b2SozO3sQLoPPRZ+hl8zsHjMrG+jLwczmm9kWM3spp1ve+W7Bd6L3stzM+v53OXvpAPX/a/Q5Wm5mD5jZqJx+86L6V5vZxf1Z67AOCDOLA3cAlwAnAx8xs5OLW1WPpIDPu/vJwFnAp6O6bwEedfdpwKPR84Hss8CqnOffAL7l7icA24FPFqWqnvs28N/u/jbg7YT3MmiWgZlNAD4D1Ln7qUAcuJqBvxx+Aszp1u1A8/0SYFr0dwNwZz/VeDA/Yf/6HwZOdffTgVeAeQDR9/pq4JRonO9F661+MawDApgNNLj7WndvA+4FLi9yTYfk7pvd/fno8W7CimkCofa7o8HuBq4oToWHZmYTgcuAH0XPDXgvsCAaZKDXXw2cB/wYwN3b3H0Hg2gZRBJAuZklgApgMwN8Obj774Ft3TofaL5fDvyHB88Ao8zsmP6pNL989bv7YndPRU+fASZGjy8H7nX3Vnd/DWggrLf6xXAPiAnAhpznjVG3QcPMpgAzgWeBce6+Oer1BjCuSGX1xO3APwKZ6PlYYEfOl2SgL4upQBPw79Fush+ZWSWDaBm4+0bgm8DrhGDYCSxlcC2HrAPN98H4Hf9r4HfR46LWP9wDYlAzsxHA/cDfufuu3H4ezl8ekOcwm9n7gS3uvrTYtRyBBHAGcKe7zwT20m130kBeBgDRfvrLCWE3Hqhk/10fg85An+8HY2ZfIOxC/nmxawEFxEZgUs7ziVG3Ac/MkoRw+Lm7/zLq/Ga2+Rz931Ks+g7hHGCuma0j7NZ7L2F//qhoVwcM/GXRCDS6+7PR8wWEwBgsywDgfcBr7t7k7u3ALwnLZjAth6wDzfdB8x03s+uA9wMf9c4L1Ipa/3APiCXAtOisjRLCwaCFRa7pkKL99T8GVrn7bTm9FgIfjx5/HPh1f9fWE+4+z90nuvsUwjx/zN0/CjwOfCgabMDWD+DubwAbzOykqNMFwEoGyTKIvA6cZWYV0Wcq+x4GzXLIcaD5vhD4q+hsprOAnTm7ogYMM5tD2OU6192bc3otBK42s1Izm0o42P5cvxXm7sP6D7iUcNbAq8AXil1PD2t+F6EJvRxYFv1dStiP/yiwBngEGFPsWnvwXs4Hfhs9Pi768DcA/wWUFru+Q9Q+A6iPlsOvgNGDbRkAXwVeBl4CfgqUDvTlANxDOGbSTmjJffJA8x0wwpmKrwIvEs7YGoj1NxCONWS/z9/PGf4LUf2rgUv6s1bdakNERPIa7ruYRETkABQQIiKSlwJCRETyUkCIiEheCggREclLASEyAJjZ+dm72ooMFAoIERHJSwEh0gtmdq2ZPWdmy8zsB9FvWuwxs29Fv6vwqJnVRsPOMLNncu7xn/2NghPM7BEz+7OZPW9mx0eTH5Hz+xI/j65uFikaBYRID5nZdODDwDnuPgNIAx8l3OSu3t1PAZ4EvhKN8h/AP3m4x/+LOd1/Dtzh7m8H3km4qhbCXXn/jvDbJMcR7oskUjSJQw8iIpELgFnAkmjjvpxwU7gM8ItomJ8Bv4x+L2KUuz8Zdb8b+C8zqwImuPsDAO7eAhBN7zl3b4yeLwOmAH8s/NsSyU8BIdJzBtzt7vO6dDT7UrfhDvf+Na05j9Po+ylFpl1MIj33KPAhMzsKOn4HeTLhe5S9++k1wB/dfSew3czOjbp/DHjSwy8ANprZFdE0Ss2sol/fhUgPaQtFpIfcfaWZfRFYbGYxwt04P034saDZUb8thOMUEG47/f0oANYCn4i6fwz4gZndGk3jL/vxbYj0mO7mKnKEzGyPu48odh0ifU27mEREJC+1IEREJC+1IEREJC8FhIiI5KWAEBGRvBQQIiKSlwJCRETy+v9GE8qS3xkj+AAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "ANNmodel=keras.models.load_model('/content/drive/MyDrive/Assignment_2_data/ANN_best.epoch-loss.hdf5')\n",
        "accuracy = CNNmodel.evaluate(x_test, y_test)\n",
        "pred=CNNmodel.predict(x_test)"
      ],
      "metadata": {
        "id": "GOndp9EJxnDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def modify_predictions(y_pred, th):\n",
        "  for arr in y_pred:\n",
        "    i = 0\n",
        "    for z in arr:\n",
        "      if(z>=th):\n",
        "        arr[i] = 1\n",
        "      else:\n",
        "        arr[i] = 0\n",
        "      i = i+1\n",
        "  return y_pred"
      ],
      "metadata": {
        "id": "lYzw02kUxs6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, modify_predictions(pred, 0.)))"
      ],
      "metadata": {
        "id": "HmaOos42xvbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "multilabel_confusion_matrix(y_test, pred)"
      ],
      "metadata": {
        "id": "TPSaK2rfxz2f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}